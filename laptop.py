# -*- coding: utf-8 -*-
"""laptop.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1h2QKeeDav5Ftyeo6nbYM4lTUnYN3UPRP

ST1/ST1G Assignment 9 (Capstone Programming Project)  Laptop_price.csv  UCC_student_u3279027StudentID
"""

from google.colab import drive
drive.mount('/content/drive')

"""### This Project is based on the Laptop Price Prediction data available from Kaggle repository (https://www.kaggle.com/datasets/mrsimple07/laptoppriceprediction/data).


*   It contains the details of 1000 Laptops price on the website.
*   My project task is to create a machine learning model which can predict the average price of the laptop based on its characteristics.
* For solving this problem, I will approach the task, with a step by step approach to create a data analysis and prediction model available from different Python packages, modules and classes.
"""

import os
os.environ['KAGGLE_CONFIG_DIR'] = '/content'

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
lap = pd.read_csv('/content/drive/MyDrive/Laptop_price.csv', encoding='latin')
print('Shape befor deleting duplicate values:', lap.shape)

# Removing duplicate rows if any
lap=lap.drop_duplicates()
print('Shape After deleting duplicate values:', lap.shape)

lap.head(10)

lap.tail(10)

"""## Key observations from Step 1 about Data Description


*   This file contains 1000 laptops details from the website.
*   There are 7 attributes and they are outlined below.
*   Brand - Based on the manufacturer of the laptop.

*   Processor_Speed - determined by how many calculations the processor can perform per cycle.
*   RAM_Size - provides high-speed, short-term memory for your computer's CPU.
*   Storage_Capacity - how much disk space one or more storage devices provides
*   Screen_Size - physical measurement of the display screen from one corner to the opposite corner, typically measured in inches.
*   Weight - how heavy the laptop is, measured in kilograms (kg).
*   Price - The value of laptops sold by the manufacturer.

# **Step 2:Problem Statement Definition**
Creating a prediction model to predict the price of a laptop
Target Variable: Price Predictors/Features: Processor_Speed, RAM_Size, Storage_Capacity, Screen_Size, Weight.

# Step 3: Choosing the appropriate ML/AI Algorithm for Data Analysis.

```
# Based on the problem statement we need to create a supervised ML Regression model, as the target variable is **Continuous**.
```

# Step 4:Looking at the class distribution (Target variable distribution to check if the data is balanced or skewed.
* If target variable's distribution is too skewed then the predictive modeling will lead to poor results.
* Ideally Bell curve is desirable but slightly positive skew or negative skew is also fine.
* When performing Regression algorithm modelling and analysis, we need to make sure the histogram looks like a bell curve or slight skewed version of it.
* Otherwise it impacts the Machine Learning algorithms ability to learn all the scenarios from the data.
"""

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline
# Creating histogram as the Target variable is Continuous
# This will help us to understand the distribution of the MEDV values
lap['Price'].hist()

"""## Observations from Step 4
* The data distribution of the target variable is satisfactory to proceed further.
* There are sufficient number of rows for each type of values to learn from.

## Step 5: Basic Exploratory Data Analysis
* There are four commands which are used for Basic data exploratory Analysis in Python

* head() : This helps to see a few sample rows of the data
* info() : This provides the summarized information of the data
* describe() : This provides the descriptive statistical details of the data
* nunique(): This helps us to identify if a column is categorical or continuous
"""

# Looking at sample rows in the data
lap.head()

# Looking at sample rows in the data
lap.tail()

# Observing the summarized information of data
# Data types, Missing values based on number of non-null values Vs total rows etc.
# Remove those variables from data which have too many missing values (Missing Values > 30%)
# Remove Qualitative variables which cannot be used in Machine Learning
lap.info()

# Looking at the descriptive statistics of the data
lap.describe(include='all')

# Finging unique values for each column
# TO understand which column is categorical and which one is Continuous
# Typically if the numer of unique values are < 20 then the variable is likely to be a category otherwise continuous
lap.nunique()

"""## Observations from Step 5 - Basic Exploratory Data Analysis
* Hence, creating a initial roadmap for further analysis.

* The selected columns in this step are not final, further study will be done and then a final list will be created

* Brand - Categorical. Selected.
* Processor_Speed - Continuous. Selected.
* RAM_Size - Categorical. Selected.
* Storage_Capacity - Categorical. Selected.
* Screen_Size - Continuous. Selected.
* Weight - Continuous. Selected.
* Price - Continuous. Selected.
* This is the Target or Class Variable, which needs to be predicted by the proposed regression model!

## Step 7: Removing Unwanted columns
* There are no qualitative columns in the data.
* Hence no need to remove any column.

## Step 8: Visual Exploratory Data Analysis
* Visualize distribution of all the Categorical Predictor variables in the data using bar plots
* We can spot a categorical variable in the data by looking at the unique values in them.
* Typically a categorical variable contains less than 20 Unique values AND there is repetition of values, which means the data can be grouped by those unique values.
* Based on the Basic Exploration Data Analysis in the previous step,  we could spotted two categorical predictors in the data

* Categorical Predictors:

* 'RAM_Size',
* 'Brand',
* 'Storage_Capacity'

* We will use bar charts to see how the data is distributed for these categorical columns.
"""

# Commented out IPython magic to ensure Python compatibility.
# Plotting multiple bar charts at once for categorical variables
# Since there is no default function which can plot bar charts for multiple columns at once
# we are defining our own function for the same

def PlotBarCharts(inpData, colsToPlot):
#     %matplotlib inline

    import matplotlib.pyplot as plt

    # Generating multiple subplots
    fig, subPlot=plt.subplots(nrows=1, ncols=len(colsToPlot), figsize=(20,5))
    fig.suptitle('Bar charts of: '+ str(colsToPlot))

    for colName, plotNumber in zip(colsToPlot, range(len(colsToPlot))):
        inpData.groupby(colName).size().plot(kind='bar',ax=subPlot[plotNumber])

#####################################################################
# Calling the function PlotBarCharts() we have created
PlotBarCharts(inpData=lap, colsToPlot=['Brand','RAM_Size','Storage_Capacity'])

"""# Step 9:"""

# Plotting histograms of multiple columns together
lap.hist(['Processor_Speed', 'Screen_Size', 'Weight', 'Price'], figsize=(18,10))

"""## Step 10: Outlier Analysis
* Outliers are extreme values in the data which are far away from most of the values.
* You can see them as the tails in the histogram.

* Outlier must be treated one column/data attribute at a time.
* As the treatment will be slightly different for each column
* Why I should analyse the outliers?
* Outliers bias the building of machine learning models.
* As the algorithm tries to fit the extreme value, it goes away from majority of the data.
* Outlined below are two options to treat outliers in the data.

* Option-1: Delete the outlier Records. Only if there are just few rows lost.
* Option-2: Impute the outlier values with a logical business value
* Let us find out out the most logical value to be replaced in place of outliers by looking at the histogram.

"""

#Replacing outliers for 'Volume'
# Finding nearest values to over 40000 (at this histogram larger than 3) mark
lap['Price'][lap['Price']<40000].sort_values(ascending=False)

"""##Step 11:Visualising Data Distribution after outlier removal"""

lap.hist(['Price'], figsize=(18,5))

"""## Step 12: Missing Values Analysis
Missing values are treated for each column separately.

If a column has more than 30% data missing, then missing value treatment cannot be done.

That column must be rejected because too much information is missing.

Outlined below are some options for treating missing values in data.

Delete the missing value rows if there are only few records

Impute the missing values with MEDIAN value for continuous variables

Impute the missing values with MODE value for categorical variables

Interpolate the values based on nearby values

Interpolate the values based on business logic
"""

# Finding how many missing values are there for each column
lap.isnull().sum()

"""##Observations from Step 12: Missing Value Analysis
* No missing values in data
* So no removal is required

## Step 13: Feature Selection (Attribute Selection)
"""

ContinuousCols=['Processor_Speed', 'Screen_Size', 'Weight']

# Plotting scatter chart for each predictor vs the target variable
for predictor in ContinuousCols:
    lap.plot.scatter(x=predictor, y='Price', figsize=(10,5), title="Price" + " affected by "+predictor)

"""## Scatter charts interpretation
* What should you look for in these scatter charts?

* **No Trend**: You cannot see any clear increasing or decreasing trend. This means there is no correlation between the variables. Hence that predictor/feature may not be the best one for ML model building.

## Step 14: Statistical Feature Selection (Continuous Vs Continuous) using Correlation value

* Pearson's correlation coefficient is a powerful metric for doing this.
* It can simply be calculated as the covariance between two features  x and  y
  (numerator) divided by the product of their standard deviations (denominator):

![image.png](data:image/png;base64,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)

* This value can be calculated only between two numeric columns
Correlation between [-1,0) means inversely proportional, the scatter plot will show a downward trend
* Correlation between (0,1] means directly proportional, the scatter plot will show a upward trend
* Correlation near {0} means No relationship, the scatter plot will show no clear trend.
* If Correlation value between two variables is > 0.5 in magnitude, it indicates good relationship the sign does not matter
* We observe the correlations between Target variable and all other predictor variables(s) to check which columns/features/predictors are actually related to the target variable in question.

# Calculating correlation matrix
ContinuousCols=['Price', 'Processor_Speed', 'Screen_Size', 'Weight']

# Creating the correlation matrix
CorrelationData=BostonData[ContinuousCols].corr()
CorrelationData
"""

# Calculating correlation matrix
ContinuousCols=['Price', 'Processor_Speed', 'Screen_Size', 'Weight']

# Creating the correlation matrix
CorrelationData=lap[ContinuousCols].corr()
CorrelationData

# Filtering only those columns where absolute correlation > 0.5 with Target Variable
# reduce the 0.5 threshold if no variable is selected
CorrelationData['Price'][abs(CorrelationData['Price']) > -0.01 ]

"""## Observations from Step 14
* Final selected Continuous columns:
'Processor_Speed', 'Screen_Size'

# Step 15:  Relationship exploration: Categorical Vs Continuous -- Box Plots
* When the target variable is Continuous and the predictor variable is Categorical we analyze the relation using Boxplots,  and
* Measure the strength of relation using Anova test.
"""

# Box plots for continuous Target Variable "Price" and Categorical predictors
CategoricalColsList=['RAM_Size', 'Brand','Storage_Capacity']

import matplotlib.pyplot as plt
fig, PlotCanvas=plt.subplots(nrows=1, ncols=len(CategoricalColsList), figsize=(18,5))

# Creating box plots for each continuous predictor against the Target Variable "Price"
for PredictorCol , i in zip(CategoricalColsList, range(len(CategoricalColsList))):
    lap.boxplot(column='Price', by=PredictorCol, figsize=(5,5), vert=True, ax=PlotCanvas[i])

"""##Observations from Step 15: Box-Plots interpretation

* What should you look for in these box plots?

* These plots gives an idea about the data distribution of continuous predictor in the Y-axis for each of the category in the X-Axis.

* If the distribution looks similar for each category(Boxes are in the same line), that means the the continuous variable has NO effect on the target variable. Hence, the variables are not correlated to each other.

* On the other hand if the distribution is different for each category(the boxes are not in same line!). It hints that these variables might be correlated with Price.

* For this datadata, both the categorical predictors looks correlated with the Target variable.

We confirm this by looking at the results of ANOVA test below

## Step 16: Statistical Feature Selection (Categorical Vs Continuous) using ANOVA test

* Analysis of variance(ANOVA) is performed to check if there is any relationship between the given continuous and categorical variable

* Assumption(H0) Null Hypothesis: There is NO relation between the given variables (i.e.
* The average(mean) values of the numeric Target variable is same for all the groups in the categorical Predictor variable)
* ANOVA Test result: Probability of H0 (Null Hypothesis being true
"""

# Defining a function to find the statistical relationship with all the categorical variables
def FunctionAnova(inpData, TargetVariable, CategoricalPredictorList):
    from scipy.stats import f_oneway

    # Creating an empty list of final selected predictors
    SelectedPredictors=[]

    print('##### ANOVA Results ##### \n')
    for predictor in CategoricalPredictorList:
        CategoryGroupLists=inpData.groupby(predictor)[TargetVariable].apply(list)
        AnovaResults = f_oneway(*CategoryGroupLists)

        # If the ANOVA P-Value is <0.05, that means we reject H0
        if (AnovaResults[1] > -0.01):
            print(predictor, 'is correlated with', TargetVariable, '| P-Value:', AnovaResults[1])
            SelectedPredictors.append(predictor)
        else:
            print(predictor, 'is NOT correlated with', TargetVariable, '| P-Value:', AnovaResults[1])

    return(SelectedPredictors)

#Calling the function to check which categorical variables are correlated with target
CategoricalPredictorList=['RAM_Size', 'Brand','Storage_Capacity']
FunctionAnova(inpData=lap,
              TargetVariable='Price',
              CategoricalPredictorList=CategoricalPredictorList)

"""##Observations from Step 16
* The results of ANOVA confirm our visual analysis using box plots above.

* All categorical variables are correlated with the Target variable.
* This is something you can guess by looking at the box plots!

* Final selected Categorical columns:

'Storage_Capacity', 'RAM_Size', 'Processor_Speed', 'Screen_Size'

## Selecting final Predictors/Features for building Machine Learning/AI model.
* Based on the extensive tests with exploratory data analysis, we can select the final features/predictors/columns for machine learning model building as:
* **'Storage_Capacity','RAM_Size', 'Processor_Speed', 'Screen_Size'**
"""

SelectedColumns=['Price', 'Storage_Capacity','RAM_Size', 'Processor_Speed', 'Screen_Size']

# Selecting final columns
DataForML=lap[SelectedColumns]
DataForML.head()

# Saving this final data subset for reference during deployment
DataForML.to_pickle('DataForML.pkl')

"""## Step 17: Data Pre-processing for Machine Learning Model Building or Model Development
* List of steps that needs to be performed on predictor variables before data can be used for machine learning

* Converting each Ordinal Categorical columns to numeric
* Converting Binary nominal Categorical columns to numeric using 1/0 mapping
* Converting all other nominal categorical columns to numeric using pd.get_dummies()
* Data Transformation (Optional): Standardization/Normalization/log/sqrt. Important if you are using distance based algorithms like KNN, or Neural Networks
* Converting the ordinal variable to numeric - In this data there is no Ordinal categorical variable.
* Converting the binary nominal variable to numeric using 1/0 mapping: There is no binary nominal variable in string format in this data

## Converting the nominal variable to numeric using get_dummies()
"""

# Treating all the nominal variables at once using dummy variables
DataForML_Numeric=pd.get_dummies(DataForML)

# Adding Target Variable to the data
DataForML_Numeric['Price']=lap['Price']

# Printing sample rows
DataForML_Numeric.head()

"""## Step 18: Machine Learning Model Development:
* Splitting the data into Training and Testing sample
* We dont use the full data for creating the model (training data).
* Some data is randomly selected and kept aside for checking how good the model is.
* This is known as Testing Data and the remaining data is called Training data on which the model is built.
* Typically 70% of data is used as Training data and the rest 30% is used as Tesing data.
"""

# Printing all the column names for our reference
DataForML_Numeric.columns

#Separate Target Variable and Predictor Variables
TargetVariable='Price'
Predictors=['Storage_Capacity','RAM_Size', 'Processor_Speed', 'Screen_Size']

X=DataForML_Numeric[Predictors].values
y=DataForML_Numeric[TargetVariable].values

# Split the data into training and testing set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=428)

"""## Step 19: Standardization/Normalization of data
* You can choose not to run this step if you want to compare the resultant accuracy of this transformation with the accuracy of raw data (Optional Step)

* However, if you are using KNN or Neural Networks, then this step becomes necessary.
"""

### Standardization of data ###
from sklearn.preprocessing import StandardScaler, MinMaxScaler
# Choose either standardization or Normalization
# On this data Min Max Normalization produced better results

# Choose between standardization and MinMAx normalization
#PredictorScaler=StandardScaler()
PredictorScaler=MinMaxScaler()

# Storing the fit object for later reference
PredictorScalerFit=PredictorScaler.fit(X)

# Generating the standardized values of X
X=PredictorScalerFit.transform(X)

# Split the data into training and testing set
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Sanity check for the sampled data
print(X_train.shape)
print(y_train.shape)
print(X_test.shape)
print(y_test.shape)

"""## Step 20: Multiple Linear Regression Algorithm For ML/AI model building"""

#Multiple Linear Regression
from sklearn.linear_model import LinearRegression
RegModel = LinearRegression()

# Printing all the parameters of Linear regression
print(RegModel)

# Creating the model on Training Data
LREG=RegModel.fit(X_train,y_train)
prediction=LREG.predict(X_test)

from sklearn import metrics
# Measuring Goodness of fit in Training data
print('R2 Value:',metrics.r2_score(y_train, LREG.predict(X_train)))

###########################################################################
print('\n##### Model Validation and Accuracy Calculations ##########')

# Printing some sample values of prediction
TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)
TestingDataResults[TargetVariable]=y_test
TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

# Printing sample prediction values
print(TestingDataResults.head())

# Calculating the error for each row
TestingDataResults['APE']=100 * ((abs(
  TestingDataResults['Price']-TestingDataResults['PredictedPrice']))/TestingDataResults['Price'])

price=np.mean(TestingDataResults['APE'])
Medianprice=np.median(TestingDataResults['APE'])

Accuracy =100 - price
MedianAccuracy=100- Medianprice
print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
print('Median Accuracy on test data:', MedianAccuracy)

# Defining a custom function to calculate accuracy
# Make sure there are no zeros in the Target variable if you are using price
def Accuracy_Score(orig,pred):
    price = np.mean(100 * (np.abs(orig-pred)/orig))
    #print('#'*70,'Accuracy:', 100-price)
    return(100-price)

# Custom Scoring price calculation
from sklearn.metrics import make_scorer
custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

# Importing cross validation function from sklearn
from sklearn.model_selection import cross_val_score

# Running 10-Fold Cross validation on a given algorithm
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""

```
# This is formatted as code
```

# Decision Tree Regressor"""

# Commented out IPython magic to ensure Python compatibility.
# Decision Trees (Multiple if-else statements!)
from sklearn.tree import DecisionTreeRegressor
RegModel = DecisionTreeRegressor(max_depth=5,criterion='friedman_mse')
# Good Range of Max_depth = 2 to 20

# Printing all the parameters of Decision Tree
print(RegModel)

# Creating the model on Training Data
DT=RegModel.fit(X_train,y_train)
prediction=DT.predict(X_test)

from sklearn import metrics
# Measuring Goodness of fit in Training data
print('R2 Value:',metrics.r2_score(y_train, DT.predict(X_train)))

# Plotting the feature importance for Top 10 most important columns
# %matplotlib inline
feature_importances = pd.Series(DT.feature_importances_, index=Predictors)
feature_importances.nlargest(10).plot(kind='barh')

###########################################################################
print('\n##### Model Validation and Accuracy Calculations ##########')

# Printing some sample values of prediction
TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)
TestingDataResults[TargetVariable]=y_test
TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

# Printing sample prediction values
print(TestingDataResults.head())

# Calculating the error for each row
TestingDataResults['APE']=100 * ((abs(
  TestingDataResults['Price']-TestingDataResults['PredictedPrice']))/TestingDataResults['Price'])

MAPE=np.mean(TestingDataResults['APE'])
MedianMAPE=np.median(TestingDataResults['APE'])

Accuracy =100 - MAPE
MedianAccuracy=100- MedianMAPE
print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
print('Median Accuracy on test data:', MedianAccuracy)

# Defining a custom function to calculate accuracy
# Make sure there are no zeros in the Target variable if you are using MAPE
def Accuracy_Score(orig,pred):
    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
    #print('#'*70,'Accuracy:', 100-MAPE)
    return(100-MAPE)

# Custom Scoring MAPE calculation
from sklearn.metrics import make_scorer
custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

# Importing cross validation function from sklearn
from sklearn.model_selection import cross_val_score

# Running 10-Fold Cross validation on a given algorithm
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""# Plotting/Visualising the Decision Tree"""

# Load libraries
from IPython.display import Image
from sklearn import tree
import pydotplus

# Create DOT data
dot_data = tree.export_graphviz(RegModel, out_file=None,
                                feature_names=Predictors)

# printing the rules
#print(dot_data)

# Draw graph
graph = pydotplus.graph_from_dot_data(dot_data)

# Show graph
Image(graph.create_png(), width=2000,height=2000)
# Double click on the graph to zoom in

"""# Random Forest Regressor"""

# Commented out IPython magic to ensure Python compatibility.
# Random Forest (Bagging of multiple Decision Trees)
from sklearn.ensemble import RandomForestRegressor
RegModel = RandomForestRegressor(max_depth=4, n_estimators=400,criterion='friedman_mse')
# Good range for max_depth: 2-10 and n_estimators: 100-1000

# Printing all the parameters of Random Forest
print(RegModel)

# Creating the model on Training Data
RF=RegModel.fit(X_train,y_train)
prediction=RF.predict(X_test)

from sklearn import metrics
# Measuring Goodness of fit in Training data
print('R2 Value:',metrics.r2_score(y_train, RF.predict(X_train)))

# Plotting the feature importance for Top 10 most important columns
# %matplotlib inline
feature_importances = pd.Series(RF.feature_importances_, index=Predictors)
feature_importances.nlargest(10).plot(kind='barh')

###########################################################################
print('\n##### Model Validation and Accuracy Calculations ##########')

# Printing some sample values of prediction
TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)
TestingDataResults[TargetVariable]=y_test
TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

# Printing sample prediction values
print(TestingDataResults.head())

# Calculating the error for each row
TestingDataResults['APE']=100 * ((abs(
  TestingDataResults['Price']-TestingDataResults['PredictedPrice']))/TestingDataResults['Price'])

MAPE=np.mean(TestingDataResults['APE'])
MedianMAPE=np.median(TestingDataResults['APE'])

Accuracy =100 - MAPE
MedianAccuracy=100- MedianMAPE
print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
print('Median Accuracy on test data:', MedianAccuracy)


# Defining a custom function to calculate accuracy
# Make sure there are no zeros in the Target variable if you are using MAPE
def Accuracy_Score(orig,pred):
    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
    #print('#'*70,'Accuracy:', 100-MAPE)
    return(100-MAPE)

# Custom Scoring MAPE calculation
from sklearn.metrics import make_scorer
custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

# Importing cross validation function from sklearn
from sklearn.model_selection import cross_val_score

# Running 10-Fold Cross validation on a given algorithm
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""# Plotting One of the Decision Tree in Random Forest Regressor"""

# Plotting a single Decision Tree from Random Forest
# Load libraries
from IPython.display import Image
from sklearn import tree
import pydotplus

# Create DOT data for the 6th Decision Tree in Random Forest
dot_data = tree.export_graphviz(RegModel.estimators_[5] , out_file=None, feature_names=Predictors)

# Draw graph
graph = pydotplus.graph_from_dot_data(dot_data)

# Show graph
Image(graph.create_png(), width=2000,height=2000)
# Double click on the graph to zoom in

"""
## Step 21: AdaBoost Algorithm For ML/AI model building"""

# Commented out IPython magic to ensure Python compatibility.
# Adaboost (Boosting of multiple Decision Trees)
from sklearn.ensemble import AdaBoostRegressor
from sklearn.tree import DecisionTreeRegressor

# Choosing Decision Tree with 6 level as the weak learner
DTR=DecisionTreeRegressor(max_depth=3)
RegModel = AdaBoostRegressor(n_estimators=100, base_estimator=DTR ,learning_rate=0.04)

# Printing all the parameters of Adaboost
print(RegModel)

# Creating the model on Training Data
AB=RegModel.fit(X_train,y_train)
prediction=AB.predict(X_test)

from sklearn import metrics
# Measuring Goodness of fit in Training data
print('R2 Value:',metrics.r2_score(y_train, AB.predict(X_train)))

# Plotting the feature importance for Top 10 most important columns
# %matplotlib inline
feature_importances = pd.Series(AB.feature_importances_, index=Predictors)
feature_importances.nlargest(10).plot(kind='barh')

###########################################################################
print('\n##### Model Validation and Accuracy Calculations ##########')

# Printing some sample values of prediction
TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)
TestingDataResults[TargetVariable]=y_test
TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

# Printing sample prediction values
print(TestingDataResults.head())

# Calculating the error for each row
TestingDataResults['APE']=100 * ((abs(
  TestingDataResults['Price']-TestingDataResults['PredictedPrice']))/TestingDataResults['Price'])

MAPE=np.mean(TestingDataResults['APE'])
MedianMAPE=np.median(TestingDataResults['APE'])

Accuracy =100 - MAPE
MedianAccuracy=100- MedianMAPE
print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
print('Median Accuracy on test data:', MedianAccuracy)


# Defining a custom function to calculate accuracy
# Make sure there are no zeros in the Target variable if you are using MAPE
def Accuracy_Score(orig,pred):
    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
    #print('#'*70,'Accuracy:', 100-MAPE)
    return(100-MAPE)

# Custom Scoring MAPE calculation
from sklearn.metrics import make_scorer
custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

# Importing cross validation function from sklearn
from sklearn.model_selection import cross_val_score

# Running 10-Fold Cross validation on a given algorithm
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""# XGBoost Regressor"""

# Commented out IPython magic to ensure Python compatibility.
# Xtreme Gradient Boosting (XGBoost)
from xgboost import XGBRegressor
RegModel=XGBRegressor(max_depth=2,
                      learning_rate=0.1,
                      n_estimators=1000,
                      objective='reg:linear',
                      booster='gbtree')

# Printing all the parameters of XGBoost
print(RegModel)

# Creating the model on Training Data
XGB=RegModel.fit(X_train,y_train)
prediction=XGB.predict(X_test)

from sklearn import metrics
# Measuring Goodness of fit in Training data
print('R2 Value:',metrics.r2_score(y_train, XGB.predict(X_train)))

# Plotting the feature importance for Top 10 most important columns
# %matplotlib inline
feature_importances = pd.Series(XGB.feature_importances_, index=Predictors)
feature_importances.nlargest(10).plot(kind='barh')
###########################################################################
print('\n##### Model Validation and Accuracy Calculations ##########')

# Printing some sample values of prediction
TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)
TestingDataResults[TargetVariable]=y_test
TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

# Printing sample prediction values
print(TestingDataResults.head())

# Calculating the error for each row
TestingDataResults['APE']=100 * ((abs(
  TestingDataResults['Price']-TestingDataResults['PredictedPrice']))/TestingDataResults['Price'])


MAPE=np.mean(TestingDataResults['APE'])
MedianMAPE=np.median(TestingDataResults['APE'])

Accuracy =100 - MAPE
MedianAccuracy=100- MedianMAPE
print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
print('Median Accuracy on test data:', MedianAccuracy)


# Defining a custom function to calculate accuracy
# Make sure there are no zeros in the Target variable if you are using MAPE
def Accuracy_Score(orig,pred):
    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
    #print('#'*70,'Accuracy:', 100-MAPE)
    return(100-MAPE)

# Custom Scoring MAPE calculation
from sklearn.metrics import make_scorer
custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

# Importing cross validation function from sklearn
from sklearn.model_selection import cross_val_score

# Running 10-Fold Cross validation on a given algorithm
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""#Plotting a single Decision tree out of XGBoost"""

#Plotting a single Decision tree out of XGBoost
from xgboost import plot_tree
import matplotlib.pyplot as plt
fig, ax = plt.subplots(figsize=(20, 8))
plot_tree(XGB, num_trees=10, ax=ax)

"""# K-Nearest Neighbor(KNN)"""

#kNN
# K-Nearest Neighbor(KNN)
from sklearn.neighbors import KNeighborsRegressor
RegModel = KNeighborsRegressor(n_neighbors=3)

# Printing all the parameters of KNN
print(RegModel)

# Creating the model on Training Data
KNN=RegModel.fit(X_train,y_train)
prediction=KNN.predict(X_test)

from sklearn import metrics
# Measuring Goodness of fit in Training data
print('R2 Value:',metrics.r2_score(y_train, KNN.predict(X_train)))

# Plotting the feature importance for Top 10 most important columns
# The variable importance chart is not available for KNN

###########################################################################
print('\n##### Model Validation and Accuracy Calculations ##########')

# Printing some sample values of prediction
TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)
TestingDataResults[TargetVariable]=y_test
TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

# Printing sample prediction values
print(TestingDataResults.head())

# Calculating the error for each row
TestingDataResults['APE']=100 * ((abs(
  TestingDataResults['Price']-TestingDataResults['PredictedPrice']))/TestingDataResults['Price'])

MAPE=np.mean(TestingDataResults['APE'])
MedianMAPE=np.median(TestingDataResults['APE'])

Accuracy =100 - MAPE
MedianAccuracy=100- MedianMAPE
print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
print('Median Accuracy on test data:', MedianAccuracy)

# Defining a custom function to calculate accuracy
# Make sure there are no zeros in the Target variable if you are using MAPE
def Accuracy_Score(orig,pred):
    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
    #print('#'*70,'Accuracy:', 100-MAPE)
    return(100-MAPE)

# Custom Scoring MAPE calculation
from sklearn.metrics import make_scorer
custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

# Importing cross validation function from sklearn
from sklearn.model_selection import cross_val_score

# Running 10-Fold Cross validation on a given algorithm
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""# Support Vector Machine (SVM) Regressor"""

# Commented out IPython magic to ensure Python compatibility.
# Support Vector Machines(SVM)
from sklearn import svm
RegModel = svm.SVR(C=50, kernel='rbf', gamma=0.01)

# Printing all the parameters
print(RegModel)

# Creating the model on Training Data
SVM=RegModel.fit(X_train,y_train)
prediction=SVM.predict(X_test)

from sklearn import metrics
# Measuring Goodness of fit in Training data
print('R2 Value:',metrics.r2_score(y_train, SVM.predict(X_train)))

# Plotting the feature importance for Top 10 most important columns
# The built in attribute SVM.coef_ works only for linear kernel
# %matplotlib inline
#feature_importances = pd.Series(SVM.coef_[0], index=Predictors)
#feature_importances.nlargest(10).plot(kind='barh')

###########################################################################
print('\n##### Model Validation and Accuracy Calculations ##########')

# Printing some sample values of prediction
TestingDataResults=pd.DataFrame(data=X_test, columns=Predictors)
TestingDataResults[TargetVariable]=y_test
TestingDataResults[('Predicted'+TargetVariable)]=np.round(prediction)

# Printing sample prediction values
print(TestingDataResults.head())

# Calculating the error for each row
TestingDataResults['APE']=100 * ((abs(
  TestingDataResults['Price']-TestingDataResults['PredictedPrice']))/TestingDataResults['Price'])

MAPE=np.mean(TestingDataResults['APE'])
MedianMAPE=np.median(TestingDataResults['APE'])

Accuracy =100 - MAPE
MedianAccuracy=100- MedianMAPE
print('Mean Accuracy on test data:', Accuracy) # Can be negative sometimes due to outlier
print('Median Accuracy on test data:', MedianAccuracy)

# Defining a custom function to calculate accuracy
# Make sure there are no zeros in the Target variable if you are using MAPE
def Accuracy_Score(orig,pred):
    MAPE = np.mean(100 * (np.abs(orig-pred)/orig))
    #print('#'*70,'Accuracy:', 100-MAPE)
    return(100-MAPE)

# Custom Scoring MAPE calculation
from sklearn.metrics import make_scorer
custom_Scoring=make_scorer(Accuracy_Score, greater_is_better=True)

# Importing cross validation function from sklearn
from sklearn.model_selection import cross_val_score

# Running 10-Fold Cross validation on a given algorithm
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""# Step 21: Model Deployment
* Deployment of the Model - Based on the above trials we select that algorithm which produces the best average accuracy.

* In this case, multiple algorithms have produced similar kind of average accuracy. Hence, we can choose any one of them.

* I am choosing XGboost as the final model it has the highest accuracy!

* In order to deploy the model we follow steps outlined next.

* Train/Build the model again using 100% data available

* Save the model as a serialized file which can be stored anywhere.

* Create a python function which gets integrated with front-end Viewer(GUI/ Website etc.) to take all the inputs and returns the prediction

* Choosing only the most important variables

* Its beneficial to keep lesser number of predictors for the model while deploying it in production.

* The lesser predictors you keep, the better it is, because the model will be less dependent on predictor columns/features, hence, more stable.

* This is important specially when the data is high dimensional(too many predictor columns/features).

* For this dataset, the most important predictor variables are 'Storage_Capacity','RAM_Size', 'Processor_Speed', 'Screen_Size'. As these are consistently on top of the variable importance chart for every algorithm. Hence choosing these as final set of predictor variables will result in better house price prediction platform/system.
"""

# Separate Target Variable and Predictor Variables
TargetVariable='Price'

# Selecting the final set of predictors for the deployment
# Based on the variable importance charts of multiple algorithms above
Predictors=['Storage_Capacity','RAM_Size', 'Processor_Speed', 'Screen_Size']

X=DataForML_Numeric[Predictors].values
y=DataForML_Numeric[TargetVariable].values

### Sandardization of data ###
from sklearn.preprocessing import StandardScaler, MinMaxScaler
# Choose either standardization or Normalization
# On this data Min Max Normalization produced better results

# Choose between standardization and MinMAx normalization
#PredictorScaler=StandardScaler()
PredictorScaler=MinMaxScaler()

# Storing the fit object for later reference
PredictorScalerFit=PredictorScaler.fit(X)

# Generating the standardized values of X
X=PredictorScalerFit.transform(X)

print(X.shape)
print(y.shape)

"""# Cross validating the final model accuracy with less predictors"""

# Importing cross validation function from sklearn
from sklearn.model_selection import cross_val_score

# choose from different tunable hyper parameters
from xgboost import XGBRegressor
RegModel=XGBRegressor(max_depth=2,
                      learning_rate=0.1,
                      n_estimators=1000,
                      objective='reg:linear',
                      booster='gbtree')

# Running 10-Fold Cross validation on a given algorithm
# Passing full data X and y because the K-fold will split the data and automatically choose train/test
Accuracy_Values=cross_val_score(RegModel, X , y, cv=10, scoring=custom_Scoring)
print('\nAccuracy values for 10-fold Cross Validation:\n',Accuracy_Values)
print('\nFinal Average Accuracy of the model:', round(Accuracy_Values.mean(),2))

"""# Step 22: Retraining the final model using 100% data"""

# Training the model on 100% Data available
Final_XGB_Model=RegModel.fit(X,y)

"""# Step 23: Save the model as a serialized file which can be stored anywhere"""

import pickle
import os

# Saving the Python objects as serialized files can be done using pickle library
# Here let us save the Final model
with open('Final_XGB_Model.pkl', 'wb') as fileWriteStream:
    pickle.dump(Final_XGB_Model, fileWriteStream)
    # Don't forget to close the filestream!
    fileWriteStream.close()

print('pickle file of Predictive Model is saved at Location:',os.getcwd())

"""# Step 24: Create a python function"""

from re import IGNORECASE
# This Function can be called from any from any front end tool/website

def FunctionPredictResult(InputData):
    import pandas as pd
    Num_Inputs=InputData.shape[0]

    # Making sure the input data has same columns as it was used for training the model
    # Also, if standardization/normalization was done, then same must be done for new input

    # Appending the new data with the Training data
    DataForML=pd.read_pickle('DataForML.pkl')
    #InputData=InputData.append(DataForML, ignore_index=True)
    InputData = pd.concat([InputData, DataForML], ignore_index=True)

    # Generating dummy variables for rest of the nominal variables
    InputData=pd.get_dummies(InputData)

    # Maintaining the same order of columns as it was during the model training
    Predictors=['Storage_Capacity','RAM_Size', 'Processor_Speed', 'Screen_Size']

    # Generating the input values to the model
    X=InputData[Predictors].values[0:Num_Inputs]

    # Generating the standardized values of X since it was done while model training also
    X=PredictorScalerFit.transform(X)

    # Loading the Function from pickle file
    import pickle
    with open('Final_XGB_Model.pkl', 'rb') as fileReadStream:
        PredictionModel=pickle.load(fileReadStream)
        # Don't forget to close the filestream!
        fileReadStream.close()

    # Genrating Predictions
    Prediction=PredictionModel.predict(X)
    PredictionResult=pd.DataFrame(Prediction, columns=['Prediction'])
    return(PredictionResult)

"""# Step 25: Calling the function for some new data"""

# Calling the function for some new data
NewSampleData=pd.DataFrame(data=[[512,16,3.83029570620903,11.1851474258484],[1000,4,2.91283295897302,11.3113717971104]],columns=['Storage_Capacity','RAM_Size', 'Processor_Speed', 'Screen_Size'])

print(NewSampleData)

# Calling the Function for prediction
FunctionPredictResult(InputData=NewSampleData)

"""# Conclusion
* The Function FunctionPredictResult() can be used to produce the predictions for one or more new cases at a time.
* Hence, it can be scheduled using a batch job or cron job to run every night and generate predictions for all the house price tasks  in the platform /system.

# Deploying a predictive model as an API
* Django and flask are two popular ways to deploy predictive models as a web service
* You can call your predictive models using a URL from any front end like tableau, java or angular js

# Deploying the model with few parameters
# Function for predictions API
"""

# Creating the function which can take inputs and return prediction
def FunctionGeneratePrediction(inp_Storage_Capacity, inp_RAM_Size, inp_Processor_Speed, inp_Screen_Size): # Changed argument name here

    # Creating a data frame for the model input
    SampleInputData=pd.DataFrame(
     data=[[inp_Storage_Capacity, inp_RAM_Size, inp_Processor_Speed, inp_Screen_Size]],
     columns=['Storage_Capacity','RAM_Size', 'Processor_Speed', 'Screen_Size'])

    # Calling the function defined above using the input parameters
    Predictions=FunctionPredictResult(InputData= SampleInputData)

    # Returning the predictions
    return(Predictions.to_json())

# Function call
FunctionGeneratePrediction( inp_Storage_Capacity=512, inp_RAM_Size=16, inp_Processor_Speed=3.83029570620903, inp_Screen_Size=11.1851474258484 ) # Argument name changed to match

"""# Web Deployment using Flask Library/Package
# Installing the flask library required to create the API

"""

!pip install flask

"""# Creating Flask API


"""

from flask import Flask, request, jsonify
import pickle
import pandas as pd
import numpy

app = Flask(__name__)

@app.route('/prediction_api', methods=["GET"])
def prediction_api():
    try:
        # Getting the paramters from API call
        Storage_Capacity_value = float(request.args.get('Storage_Capacity'))
        RAM_Size_value = float(request.args.get('RAM_Size'))
        Processor_Speed_value = float(request.args.get('Processor_Speed'))
        Screen_Size_value = float(request.args.get('Screen_Size'))

        # Calling the funtion to get predictions
        prediction_from_api=FunctionGeneratePrediction(
                                                       inp_Storage_Capacity=Storage_Capacity_value,
                                                       inp_RAM_Size=RAM_Size_value,
                                                       inp_Processor_Speed=Processor_Speed_value,
                                                       inp_Screen_Size=Screen_Size_value
                                                )

        return (prediction_from_api)

    except Exception as e:
        return('Something is not right!:'+str(e))

"""# Starting the API engine"""

import os
if __name__ =="__main__":

    # Hosting the API in localhost
    app.run(host='127.0.0.2', port=9000, threaded=True, debug=True, use_reloader=False)
    # Interrupt kernel to stop the API